<div align="center">
  <p>
    <a href="https://github.com/CVHub520/X-AnyLabeling/" target="_blank">
      <img width="100%" src="https://user-images.githubusercontent.com/72010077/273420485-bdf4a930-8eca-4544-ae4b-0e15f3ebf095.png"></a>
  </p>

[English](README.md) | [ÁÆÄ‰Ωì‰∏≠Êñá](README_zh-CN.md)

</div>

<p align="center">
    <a href="./LICENSE"><img src="https://img.shields.io/badge/License-LGPL%20v3-blue.svg"></a>
    <a href=""><img src="https://img.shields.io/github/v/release/CVHub520/X-AnyLabeling?color=ffa"></a>
    <a href=""><img src="https://img.shields.io/badge/python-3.8+-aff.svg"></a>
    <a href=""><img src="https://img.shields.io/badge/os-linux%2C%20win%2C%20mac-pink.svg"></a>
    <a href="https://github.com/CVHub520/X-AnyLabeling/stargazers"><img src="https://img.shields.io/github/stars/CVHub520/X-AnyLabeling?color=ccf"></a>
</p>

![](https://user-images.githubusercontent.com/18329471/234640541-a6a65fbc-d7a5-4ec3-9b65-55305b01a7aa.png)

<video src="https://github.com/user-attachments/assets/6e5131af-6946-4586-93c4-d1b8540e9e33" 
       controls 
       width="100%" 
       height="auto" 
       style="max-width: 720px; height: auto; display: block; object-fit: contain;">
</video>

## ü•≥ What's New

- Aug. 2024:
  - ‚ú®‚ú®‚ú® Support [Segment-Anything-2](https://docs.ultralytics.com/tasks/pose/) model! (Recommended)
  - ü§ó Pre-release the latest version [2.4.0](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.4.0) ü§ó
- Jul. 2024:
  - Add PPOCR-Recognition and KIE import/export functionality for training PP-OCR task.
  - Add ODVG import/export functionality for training grounding task.
  - Add support to annotate KIE linking field.
  - Support [RT-DETRv2](https://github.com/lyuwenyu/RT-DETR) model.
  - Support [Depth Anything v2](https://github.com/DepthAnything/Depth-Anything-V2) model.
- Jun. 2024:
  - Support [YOLOv8-Pose](https://docs.ultralytics.com/tasks/pose/) model.
  - Add [yolo-pose](./docs/en/user_guide.md) import/export functionality.
- May. 2024:
  - Support [YOLOv8-World](https://docs.ultralytics.com/models/yolo-world), [YOLOv8-oiv7](https://docs.ultralytics.com/models/yolov8), [YOLOv10](https://github.com/THU-MIG/yolov10) model.
  - Release version [2.3.6](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.3.6).
  - Add feature to display confidence score.
- Mar. 2024:
  - Release version [2.3.5](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.3.5).
- Feb. 2024:
  - Release version [2.3.4](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.3.4).
  - Enable label display feature.
  - Release version [2.3.3](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.3.3).
  - Release version [2.3.2](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.3.2).
  - Support [YOLOv9](https://github.com/WongKinYiu/yolov9) model.
  - Support the conversion from a horizontal bounding box to a rotated bounding box.
  - Supports label deletion and renaming. For more details, please refer to the [document](./docs/zh_cn/user_guide.md).
  - Support for quick tag correction is available; please refer to this [document](./docs/en/user_guide.md) for guidance.
  - Release version [2.3.1](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.3.1).
- Jan. 2024:
  - üëèüëèüëè Combining CLIP and SAM models for enhanced semantic and spatial understanding. An example can be found [here](./anylabeling/configs/auto_labeling/edge_sam_with_chinese_clip.yaml).
  - üî•üî•üî• Adding support for the [Depth Anything](https://github.com/LiheYoung/Depth-Anything.git) model in the depth estimation task.
  - Release version [2.3.0](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.3.0).
  - Support [YOLOv8-OBB](https://github.com/ultralytics/ultralytics) model.
  - Support [RTMDet](https://github.com/open-mmlab/mmyolo/tree/main/configs/rtmdet) and [RTMO](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose) model.
  - Release a [chinese license plate](https://github.com/we0091234/Chinese_license_plate_detection_recognition) detection and recognition model based on YOLOv5.
- Dec. 2023:
  - Release version [2.2.0](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.2.0).
  - Support [EdgeSAM](https://github.com/chongzhou96/EdgeSAM) to optimize for efficient execution on edge devices with minimal performance compromise.
  - Support YOLOv5-Cls and YOLOv8-Cls model.
- Nov. 2023:
  - Release version [2.1.0](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.1.0).
  - Support [InternImage](https://arxiv.org/abs/2211.05778) model (**CVPR'23**).
  - Release version [2.0.0](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v2.0.0).
  - Added support for Grounding-SAM, combining [GroundingDINO](https://github.com/wenyi5608/GroundingDINO) with [HQ-SAM](https://github.com/SysCV/sam-hq) to achieve sota zero-shot high-quality predictions!
  - Enhanced support for [HQ-SAM](https://github.com/SysCV/sam-hq) model to achieve high-quality mask predictions.
  - Support the [PersonAttribute](https://github.com/PaddlePaddle/PaddleClas/blob/release%2F2.5/docs/en/PULC/PULC_person_attribute_en.md) and [VehicleAttribute](https://github.com/PaddlePaddle/PaddleClas/blob/release%2F2.5/docs/en/PULC/PULC_vehicle_attribute_en.md) model for multi-label classification task.
  - Introducing a new multi-label attribute annotation functionality.
  - Release version [1.1.0](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v1.1.0).
  - Support pose estimation: [YOLOv8-Pose](https://github.com/ultralytics/ultralytics).
  - Support object-level tag with yolov5_ram.
  - Add a new feature enabling batch labeling for arbitrary unknown categories based on Grounding-DINO.
- Oct. 2023:
  - Release version [1.0.0](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v1.0.0).
  - Add a new feature for rotation box.
  -  Support [YOLOv5-OBB](https://github.com/hukaixuan19970627/yolov5_obb) with [DroneVehicle](https://github.com/VisDrone/DroneVehicle) and [DOTA](https://captain-whu.github.io/DOTA/index.html)-v1.0/v1.5/v2.0 model.
  - SOTA Zero-Shot Object Detection - [GroundingDINO](https://github.com/wenyi5608/GroundingDINO) is released.
  - SOTA Image Tagging Model - [Recognize Anything](https://github.com/xinyu1205/Tag2Text) is released.
  - Support YOLOv5-SAM and YOLOv8-EfficientViT_SAM union task.
  - Support YOLOv5 and YOLOv8 segmentation task.
  - Release [Gold-YOLO](https://github.com/huawei-noah/Efficient-Computing/tree/master/Detection/Gold-YOLO) and [DAMO-YOLO](https://github.com/tinyvision/DAMO-YOLO) models.
  - Release MOT algorithms: [OC_Sort](https://github.com/noahcao/OC_SORT) (**CVPR'23**).
  - Add a new feature for small object detection using [SAHI](https://github.com/obss/sahi).
- Sep. 2023:
  - Release version [0.2.4](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v0.2.4).
  - Release [EfficientViT-SAM](https://github.com/mit-han-lab/efficientvit) (**ICCV'23**),[SAM-Med2D](https://github.com/OpenGVLab/SAM-Med2D), [MedSAM](https://arxiv.org/abs/2304.12306) and YOLOv5-SAM.
  - Support [ByteTrack](https://github.com/ifzhang/ByteTrack) (**ECCV'22**) for MOT task.
  - Support [PP-OCRv4](https://github.com/PaddlePaddle/PaddleOCR) model.
  - Add `video` annotation feature.
  - Add `yolo`/`coco`/`voc`/`mot`/`dota` export functionality.
  - Add the ability to process all images at once.
- Aug. 2023:
  - Release version [0.2.0]((https://github.com/CVHub520/X-AnyLabeling/releases/tag/v0.2.0)).
  - Release [LVMSAM](https://arxiv.org/abs/2306.11925) and it's variants [BUID](https://github.com/CVHub520/X-AnyLabeling/tree/main/assets/examples/buid), [ISIC](https://github.com/CVHub520/X-AnyLabeling/tree/main/assets/examples/isic), [Kvasir](https://github.com/CVHub520/X-AnyLabeling/tree/main/assets/examples/kvasir).
  - Support lane detection algorithm: [CLRNet](https://github.com/Turoad/CLRNet) (**CVPR'22**).
  - Support 2D human whole-body pose estimation: [DWPose](https://github.com/IDEA-Research/DWPose/tree/main) (**ICCV'23 Workshop**).
- Jul. 2023:
  - Add [label_converter.py](./tools/label_converter.py) script.
  - Release [RT-DETR](https://github.com/PaddlePaddle/PaddleDetection/blob/develop/configs/rtdetr/README.md) model.
- Jun. 2023:
  - Release [YOLO-NAS](https://github.com/Deci-AI/super-gradients/tree/master) model.
  - Support instance segmentation: [YOLOv8-seg](https://github.com/ultralytics/ultralytics).
  - Add [README_zh-CN.md](README_zh-CN.md) of X-AnyLabeling.
- May. 2023:
  - Release version [0.1.0](https://github.com/CVHub520/X-AnyLabeling/releases/tag/v0.1.0).
  - Release [YOLOv6-Face](https://github.com/meituan/YOLOv6/tree/yolov6-face) for face detection and facial landmark detection.
  - Release [SAM](https://arxiv.org/abs/2304.02643) and it's faster version [MobileSAM](https://arxiv.org/abs/2306.14289).
  - Release [YOLOv5](https://github.com/ultralytics/yolov5), [YOLOv6](https://github.com/meituan/YOLOv6), [YOLOv7](https://github.com/WongKinYiu/yolov7), [YOLOv8](https://github.com/ultralytics/ultralytics), [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX).


## üëã Brief Introduction

`X-AnyLabeling` stands out as a robust annotation tool seamlessly incorporating an AI inference engine alongside an array of sophisticated features. Tailored for practical applications, it is committed to delivering comprehensive, industrial-grade solutions for image data engineers. This tool excels in swiftly and automatically executing annotations across diverse and intricate tasks.


## üî• Highlight

### üóùÔ∏èKey Features

- Supports inference acceleration using `GPU`.
- Handles both `image` and `video` processing.
- Allows single-frame and batch predictions for all tasks.
- Facilitates customization of models and supports secondary development design.
- Enables one-click import and export of mainstream label formats such as COCO, VOC, YOLO, DOTA, MOT, and MASK.
- Covers a range of visual tasks, including `classification`, `detection`, `segmentation`, `caption`, `rotation`, `tracking`, `estimation`, and `ocr`.
- Supports various image annotation styles, including `polygons`, `rectangles`, `rotated boxes`, `circles`, `lines`, `points`, as well as annotations for `text detection`, `recognition`, and `KIE`.


### ‚õèÔ∏èModel Zoo

<div align="center">

| **Object Detection** | **SOD with [SAHI](https://github.com/obss/sahi)** | **Facial Landmark Detection** | **2D Pose Estimation** |
| :---: | :---: | :---: | :---: |
| <img src='https://user-images.githubusercontent.com/72010077/273488633-fc31da5c-dfdd-434e-b5d0-874892807d95.png' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/61035602/206095892-934be83a-f869-4a31-8e52-1074184149d1.jpg' height="126px" width="180px"> |  <img src='https://user-images.githubusercontent.com/61035602/206095684-72f42233-c9c7-4bd8-9195-e34859bd08bf.jpg' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/61035602/206100220-ab01d347-9ff9-4f17-9718-290ec14d4205.gif' height="126px" width="180px"> |
|  **2D Lane Detection** | **OCR** | **MOT** | **Instance Segmentation** |
| <img src='https://user-images.githubusercontent.com/72010077/273764641-65f456ed-27ce-4077-8fce-b30db093b988.jpg' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/72010077/273421210-30d20e08-3b72-4f4d-8976-05b564e13d87.png' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/61035602/206111753-836e7827-968e-4c80-92ef-7a78766892fc.gif' height="126px" width="180px"  > | <img src='https://user-images.githubusercontent.com/61035602/206095831-cc439557-1a23-4a99-b6b0-b6f2e97e8c57.jpg' height="126px" width="180px"> |
|  **Image Tagging** | **Grounding DINO** | **Recognition** | **Rotation** |
| <img src='https://user-images.githubusercontent.com/72010077/277670825-8797ac7e-e593-45ea-be6a-65c3af17b12b.png' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/72010077/277395884-4d500af3-3e4e-4fb3-aace-9a56a09c0595.png' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/72010077/277396071-79daec2c-6b0a-4d42-97cf-69fd098b3400.png' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/72010077/277395955-aab54ea0-88f5-41af-ab0a-f4158a673f5e.png' height="126px" width="180px"> |
|  **[SAM](https://segment-anything.com/)** | **BC-SAM** | **Skin-SAM** | **Polyp-SAM** |
| <img src='https://user-images.githubusercontent.com/72010077/273421331-2c0858b5-0b92-405b-aae6-d061bc25aa3c.png' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/72010077/273764259-718dce97-d04d-4629-b6d2-95f17670ce2a.png' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/72010077/273764288-e26767d1-3c44-45cb-a72e-124efb4e8263.png' height="126px" width="180px"> | <img src='https://user-images.githubusercontent.com/72010077/273764318-e8b6a197-e733-478e-a210-e4386bafa1e4.png' height="126px" width="180px"> |

For more details, please refer to üëâ [model_zoo](./docs/en/model_zoo.md) üëà

</div>

## üìã Usage [‚èèÔ∏è](#üìÑ-table-of-contents)

- ### üìúDocs

  - ##### üîú[Quick Start](./docs/en/get_started.md)

  - ##### üìã[User Guide](./docs/en/user_guide.md)

  - ##### üöÄ[Load Custom Model](./docs/en/custom_model.md)

- ### üß∑Hotkeys

<details>

<summary>Click to Expand/Collapse</summary>

| Shortcut          | Function                                |
|-------------------|-----------------------------------------|
| d                 | Open next file                          |
| a                 | Open previous file                      |
| Ctrl + Shift + d  | Open next labeled file                  |
| Ctrl + Shift + a  | Open previous labeled file              |
| p or [Ctrl+n]     | Create polygon                          |
| o                 | Create rotation                         |
| r or [Crtl+r]     | Create rectangle                        |
| i                 | Run model                               |
| q                 | `positive point` of SAM mode            |
| e                 | `negative point` of SAM mode            |
| b                 | Quickly clear points of SAM mode        |
| f                 | Quickly confirm target of SAM mode      |
| g                 | Group selected shapes                   |
| u                 | Ungroup selected shapes                 |
| s                 | Hide selected shapes                    |
| w                 | Show selected shapes                    |
| Ctrl + q          | Quit                                    |
| Ctrl + i          | Open image file                         |
| Ctrl + o          | Open video file                         |
| Ctrl + u          | Load all images from a directory        |
| Ctrl + e          | Edit label                              |
| Ctrl + j          | Edit polygon                            |
| Ctrl + c          | Copy selected shapes                    |
| Ctrl + v          | Paste selected shapes                   |
| Ctrl + d          | Duplicate polygon                       |
| Ctrl + g          | Display overview annotation statistics  |
| Ctrl + h          | Toggle visibility shapes                |
| Ctrl + p          | Toggle keep previous mode               |
| Ctrl + y          | Toggle auto use last label              |
| Ctrl + m          | Run all images at once                  |
| Ctrl + a          | Enable auto annotation                  |
| Ctrl + s          | Save current annotation                 |
| Ctrl + l          | Toggle visibility Labels                |
| Ctrl + t          | Toggle visibility Texts                 |
| Ctrl + Shift + s  | Change output directory                 |
| Ctrl -            | Zoom out                                |
| Ctrl + 0          | Zoom to Original                        |
| [Ctrl++, Ctrl+=]  | Zoom in                                 |
| Ctrl + f          | Fit window                              |
| Ctrl + Shift + f  | Fit width                               |
| Ctrl + z          | Undo the last operation                 |
| Ctrl + Delete     | Delete file                             |
| Delete            | Delete polygon                          |
| Esc               | Cancel the selected object              |
| Backspace         | Remove selected point                   |
| ‚Üë‚Üí‚Üì‚Üê              | Keyboard arrows to move selected object |
| zxcv              | Keyboard to rotate selected rect box    |


</details>


## üìß Contact [‚èèÔ∏è](#üìÑ-table-of-contents)

<p align="center">
ü§ó Enjoying this project? Please give it a star! ü§ó
</p>

If you find this project helpful or interesting, consider starring it to show your support, and if you have any questions or encounter any issues while using this project, feel free to reach out for assistance using the following methods:

- [Create an issue](https://github.com/CVHub520/X-AnyLabeling/issues)
- Email: cv_hub@163.com


## ‚úÖ License [‚èèÔ∏è](#üìÑ-table-of-contents)

This project is released under the [GPL-3.0 license](./LICENSE).

## üôèüèª Acknowledgments [‚èèÔ∏è](#üìÑ-table-of-contents)

I extend my heartfelt thanks to the developers and contributors of the projects [LabelMe](https://github.com/wkentaro/labelme), [LabelImg](https://github.com/tzutalin/labelIm), [roLabelImg](https://github.com/cgvict/roLabelImg), [AnyLabeling](https://github.com/vietanhdev/anylabeling), [PPOCRLabel](https://github.com/PFCCLab/PPOCRLabel) and [Computer Vision Annotation Tool](https://github.com/opencv/cvat). Their dedication and contributions have played a crucial role in shaping the success of this project.

## üè∑Ô∏è Citing [‚èèÔ∏è](#üìÑ-table-of-contents)

### BibTeX

If you use this software in your research, please cite it as below:

```
@misc{X-AnyLabeling,
  year = {2023},
  author = {Wei Wang},
  publisher = {Github},
  organization = {CVHub},
  journal = {Github repository},
  title = {Advanced Auto Labeling Solution with Added Features},
  howpublished = {\url{https://github.com/CVHub520/X-AnyLabeling}}
}
```

<div align="right"><a href="#top">üîù Back to Top</a></div>
